{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","mount_file_id":"13hr5FCcVKIFcHipT2d4QbkdCD1A7P579","authorship_tag":"ABX9TyNBVChM93fVGr4ORq+h2jnQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install wandb -qU"],"metadata":{"id":"4HS9esIZ71s3"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"rEpOF49jC4ZG","executionInfo":{"status":"ok","timestamp":1733145975349,"user_tz":-540,"elapsed":15969,"user":{"displayName":"함양훈","userId":"11408684467918551547"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","pd.set_option('display.max_rows', None)\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizer, BertModel\n","from transformers import AdamW\n","import torch.nn as nn\n","import xgboost as xgb\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report\n","from tqdm import tqdm\n","# Log in to your W&B account\n","import wandb\n","import random\n","import math"]},{"cell_type":"markdown","source":["# main code"],"metadata":{"id":"M-qmmujh8r51"}},{"cell_type":"code","source":["import wandb\n","# 처음 실행시 WandB 웹사이트에서 발급받은 API 키를 입력해야 합니다\n","wandb.login()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"yZOFcRIO_Ofc","executionInfo":{"status":"ok","timestamp":1733146009276,"user_tz":-540,"elapsed":3170,"user":{"displayName":"함양훈","userId":"11408684467918551547"}},"outputId":"fd67faea-9e38-469f-fb99-83c80eef4f68"},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["df = pd.read_csv('/content/drive/MyDrive/University/4-2/정보기술학회/data/medical_data.csv', encoding='utf-8')\n","df.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lW3v6VrxDnSI","executionInfo":{"status":"ok","timestamp":1733142542021,"user_tz":-540,"elapsed":40617,"user":{"displayName":"함양훈","userId":"11408684467918551547"}},"outputId":"a194151a-4598-4dc5-f821-fc8d3b04ef50"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2891197, 11)"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["import logging\n","import wandb\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","from dataclasses import dataclass, field\n","from typing import Dict, List, Tuple, Optional\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","\n","@dataclass\n","class DataConfig:\n","    \"\"\"데이터 처리 관련 설정을 관리하는 클래스\"\"\"\n","    min_samples_per_class: int = 1000  # 클래스당 최소 샘플 수\n","    valid_department_threshold: int = 1000  # 유효한 진료과로 판단할 최소 샘플 수\n","    valid_disease_threshold: int = 10000  # 유효한 질병 코드로 판단할 최소 샘플 수\n","    test_size: float = 0.2\n","    random_state: int = 42\n","    text_column: str = '증상'\n","    dept_column: str = '진료과목코드'\n","    disease_column: str = '주상병코드'\n","\n","class MedicalDataProcessor:\n","    \"\"\"의료 데이터 전처리 및 균형화를 담당하는 클래스\"\"\"\n","\n","    def __init__(self, config: DataConfig):\n","        self.config = config\n","        self.logger = self._setup_logger()\n","        self.dept_encoder = LabelEncoder()\n","        self.disease_encoder = LabelEncoder()\n","        wandb.init(project=\"medical-recommendation\", config=config.__dict__)\n","\n","    def _setup_logger(self) -> logging.Logger:\n","        \"\"\"로깅 설정\"\"\"\n","        logger = logging.getLogger(__name__)\n","        logger.setLevel(logging.INFO)\n","        handler = logging.StreamHandler()\n","        handler.setFormatter(logging.Formatter(\n","            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n","        ))\n","        logger.addHandler(handler)\n","        return logger\n","\n","    def process_data(self, file_path: Path) -> Tuple[pd.DataFrame, pd.DataFrame]:\n","        \"\"\"전체 데이터 처리 파이프라인\"\"\"\n","        try:\n","            # 데이터 로드\n","            df = self._load_data(file_path)\n","\n","            # 데이터 전처리\n","            df = self._preprocess_data(df)\n","\n","            # 유효한 클래스만 선택\n","            df = self._filter_valid_classes(df)\n","\n","            # 데이터 균형화\n","            df_balanced = self._balance_data(df)\n","\n","            # 학습/테스트 분할\n","            train_df, test_df = self._split_data(df_balanced)\n","\n","            # 처리 결과 로깅\n","            self._log_processing_results(df, df_balanced, train_df, test_df)\n","\n","            return train_df, test_df\n","\n","        except Exception as e:\n","            self.logger.error(f\"Data processing failed: {str(e)}\")\n","            raise\n","\n","    def _load_data(self, file_path: Path) -> pd.DataFrame:\n","        \"\"\"데이터 로드 및 기본 검증\"\"\"\n","        self.logger.info(f\"Loading data from {file_path}\")\n","        df = pd.read_csv(file_path)\n","\n","        # 필수 컬럼 존재 확인\n","        required_columns = [self.config.text_column,\n","                          self.config.dept_column,\n","                          self.config.disease_column]\n","\n","        missing_columns = [col for col in required_columns if col not in df.columns]\n","        if missing_columns:\n","            raise ValueError(f\"Missing required columns: {missing_columns}\")\n","\n","        return df\n","\n","    def _preprocess_data(self, df: pd.DataFrame) -> pd.DataFrame:\n","        \"\"\"데이터 전처리\"\"\"\n","        # 텍스트 전처리\n","        df[self.config.text_column] = df[self.config.text_column].apply(\n","            self._preprocess_text\n","        )\n","\n","        # 레이블 인코딩\n","        df['dept_encoded'] = self.dept_encoder.fit_transform(\n","            df[self.config.dept_column]\n","        )\n","        df['disease_encoded'] = self.disease_encoder.fit_transform(\n","            df[self.config.disease_column]\n","        )\n","\n","        return df\n","\n","    def _preprocess_text(self, text: str) -> str:\n","        \"\"\"텍스트 전처리\"\"\"\n","        # 기본적인 텍스트 클리닝\n","        text = text.lower().strip()\n","\n","        # 불필요한 공백 제거\n","        text = ' '.join(text.split())\n","\n","        return text\n","\n","    def _filter_valid_classes(self, df: pd.DataFrame) -> pd.DataFrame:\n","        \"\"\"유효한 클래스만 선택\"\"\"\n","        # 진료과 기준 필터링\n","        dept_counts = df[self.config.dept_column].value_counts()\n","        valid_depts = dept_counts[dept_counts >= self.config.valid_department_threshold].index\n","\n","        # 질병 코드 기준 필터링\n","        disease_counts = df[self.config.disease_column].value_counts()\n","        valid_diseases = disease_counts[disease_counts >= self.config.valid_disease_threshold].index\n","\n","        # 유효한 클래스만 선택\n","        mask = (df[self.config.dept_column].isin(valid_depts)) & \\\n","               (df[self.config.disease_column].isin(valid_diseases))\n","\n","        return df[mask].reset_index(drop=True)\n","\n","    def _balance_data(self, df: pd.DataFrame) -> pd.DataFrame:\n","        \"\"\"클래스 별 샘플 수 균형화 - 개선된 버전\"\"\"\n","        # 최소 기준 샘플 수 결정\n","        min_samples = self.config.min_samples_per_class\n","\n","        # 각 클래스별 현재 샘플 수 확인\n","        disease_counts = df[self.config.disease_column].value_counts()\n","\n","        # 충분한 샘플을 가진 클래스만 선택\n","        valid_diseases = disease_counts[disease_counts >= min_samples].index\n","\n","        balanced_dfs = []\n","        for disease in valid_diseases:\n","            disease_df = df[df[self.config.disease_column] == disease]\n","\n","            # 정확히 min_samples만큼 샘플링\n","            sampled_df = disease_df.sample(\n","                n=min_samples,\n","                random_state=self.config.random_state\n","            )\n","            balanced_dfs.append(sampled_df)\n","\n","        balanced_df = pd.concat(balanced_dfs, axis=0).reset_index(drop=True)\n","\n","        # 결과 로깅\n","        self.logger.info(f\"Original class distribution:\\n{disease_counts}\")\n","        self.logger.info(f\"Balanced class distribution:\\n{balanced_df[self.config.disease_column].value_counts()}\")\n","\n","        return balanced_df\n","\n","    def _split_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n","        \"\"\"학습/테스트 데이터 분할\"\"\"\n","        train_df, test_df = train_test_split(\n","            df,\n","            test_size=self.config.test_size,\n","            stratify=df['dept_encoded'],\n","            random_state=self.config.random_state\n","        )\n","\n","        return train_df, test_df\n","\n","    def _log_processing_results(self, original_df: pd.DataFrame,\n","                              balanced_df: pd.DataFrame,\n","                              train_df: pd.DataFrame,\n","                              test_df: pd.DataFrame):\n","        \"\"\"처리 결과를 WandB에 기록\"\"\"\n","        wandb.log({\n","            \"data_processing\": {\n","                \"original_samples\": len(original_df),\n","                \"balanced_samples\": len(balanced_df),\n","                \"training_samples\": len(train_df),\n","                \"test_samples\": len(test_df),\n","                \"dept_distribution\": train_df[self.config.dept_column].value_counts().to_dict(),\n","                \"disease_distribution\": train_df[self.config.disease_column].value_counts().to_dict(),\n","                \"text_length_stats\": train_df[self.config.text_column].str.len().describe().to_dict()\n","            }\n","        })\n","\n","# 사용 예시\n","if __name__ == \"__main__\":\n","    # 설정 객체 생성\n","    config = DataConfig(\n","        min_samples_per_class=2000,  # 더 작은 값으로 조정\n","        valid_department_threshold=2000,  # 더 작은 값으로 조정\n","        valid_disease_threshold=2000  # 더 작은 값으로 조정\n","    )\n","\n","    # 데이터 처리기 초기화\n","    processor = MedicalDataProcessor(config)\n","\n","    # 데이터 처리 실행\n","    train_df, test_df = processor.process_data(Path(\"/content/drive/MyDrive/University/4-2/정보기술학회/data/medical_data.csv\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"GBE_Kx34_gRX","executionInfo":{"status":"ok","timestamp":1733146086436,"user_tz":-540,"elapsed":67428,"user":{"displayName":"함양훈","userId":"11408684467918551547"}},"outputId":"b5af7e26-c4f8-406a-db93-998975a4ee2b"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myanghoonham\u001b[0m (\u001b[33myanghoonham-kangnam-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.18.7"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20241202_132658-f3uc0hg5</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/yanghoonham-kangnam-university/medical-recommendation/runs/f3uc0hg5' target=\"_blank\">earnest-dawn-5</a></strong> to <a href='https://wandb.ai/yanghoonham-kangnam-university/medical-recommendation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/yanghoonham-kangnam-university/medical-recommendation' target=\"_blank\">https://wandb.ai/yanghoonham-kangnam-university/medical-recommendation</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/yanghoonham-kangnam-university/medical-recommendation/runs/f3uc0hg5' target=\"_blank\">https://wandb.ai/yanghoonham-kangnam-university/medical-recommendation/runs/f3uc0hg5</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2024-12-02 13:27:01,059 - __main__ - INFO - Loading data from /content/drive/MyDrive/University/4-2/정보기술학회/data/medical_data.csv\n","INFO:__main__:Loading data from /content/drive/MyDrive/University/4-2/정보기술학회/data/medical_data.csv\n","2024-12-02 13:28:05,999 - __main__ - INFO - Original class distribution:\n","주상병코드\n","J20    639627\n","J30    217760\n","J06    160795\n","K21    134565\n","J01    134213\n","J03    132331\n","J02    130681\n","K29    124808\n","A09    108414\n","J00    106372\n","H10    100649\n","J04     85673\n","J32     70284\n","H04     61048\n","H52     56800\n","H66     52095\n","H00     41873\n","H35     37354\n","H60     36509\n","K59     35927\n","H16     35571\n","J45     33932\n","H25     33635\n","J18     33405\n","J21     30831\n","I20     28551\n","K58     27349\n","I63     23962\n","J31     20661\n","H65     19335\n","K64     17441\n","K25     16643\n","K30     16266\n","J36     15795\n","H01     14318\n","H11     13631\n","I48     13593\n","A04     12946\n","J37     11316\n","Name: count, dtype: int64\n","INFO:__main__:Original class distribution:\n","주상병코드\n","J20    639627\n","J30    217760\n","J06    160795\n","K21    134565\n","J01    134213\n","J03    132331\n","J02    130681\n","K29    124808\n","A09    108414\n","J00    106372\n","H10    100649\n","J04     85673\n","J32     70284\n","H04     61048\n","H52     56800\n","H66     52095\n","H00     41873\n","H35     37354\n","H60     36509\n","K59     35927\n","H16     35571\n","J45     33932\n","H25     33635\n","J18     33405\n","J21     30831\n","I20     28551\n","K58     27349\n","I63     23962\n","J31     20661\n","H65     19335\n","K64     17441\n","K25     16643\n","K30     16266\n","J36     15795\n","H01     14318\n","H11     13631\n","I48     13593\n","A04     12946\n","J37     11316\n","Name: count, dtype: int64\n","2024-12-02 13:28:06,010 - __main__ - INFO - Balanced class distribution:\n","주상병코드\n","J20    2000\n","J30    2000\n","J06    2000\n","K21    2000\n","J01    2000\n","J03    2000\n","J02    2000\n","K29    2000\n","A09    2000\n","J00    2000\n","H10    2000\n","J04    2000\n","J32    2000\n","H04    2000\n","H52    2000\n","H66    2000\n","H00    2000\n","H35    2000\n","H60    2000\n","K59    2000\n","H16    2000\n","J45    2000\n","H25    2000\n","J18    2000\n","J21    2000\n","I20    2000\n","K58    2000\n","I63    2000\n","J31    2000\n","H65    2000\n","K64    2000\n","K25    2000\n","K30    2000\n","J36    2000\n","H01    2000\n","H11    2000\n","I48    2000\n","A04    2000\n","J37    2000\n","Name: count, dtype: int64\n","INFO:__main__:Balanced class distribution:\n","주상병코드\n","J20    2000\n","J30    2000\n","J06    2000\n","K21    2000\n","J01    2000\n","J03    2000\n","J02    2000\n","K29    2000\n","A09    2000\n","J00    2000\n","H10    2000\n","J04    2000\n","J32    2000\n","H04    2000\n","H52    2000\n","H66    2000\n","H00    2000\n","H35    2000\n","H60    2000\n","K59    2000\n","H16    2000\n","J45    2000\n","H25    2000\n","J18    2000\n","J21    2000\n","I20    2000\n","K58    2000\n","I63    2000\n","J31    2000\n","H65    2000\n","K64    2000\n","K25    2000\n","K30    2000\n","J36    2000\n","H01    2000\n","H11    2000\n","I48    2000\n","A04    2000\n","J37    2000\n","Name: count, dtype: int64\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from transformers import AutoTokenizer, AutoModel\n","import xgboost as xgb\n","from dataclasses import dataclass, field\n","from typing import Dict, List, Optional, Tuple\n","import numpy as np\n","from sklearn.metrics import accuracy_score, f1_score\n","import wandb\n","\n","@dataclass\n","class ModelConfig:\n","    \"\"\"모델 구성을 위한 설정 클래스\"\"\"\n","    # 기본 설정\n","    seed: int = 42\n","    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","    # BERT 트랙 설정\n","    bert_model_name: str = \"madatnlp/km-bert\"\n","    tokenizer_name: str = \"snunlp/KR-BERT-char16424\"\n","    max_length: int = 512\n","    bert_batch_size: int = 32\n","    bert_learning_rate: float = 2e-5\n","    bert_epochs: int = 10\n","    warmup_ratio: float = 0.1\n","\n","    # XGBoost 트랙 설정\n","    xgb_params: Dict = field(default_factory=lambda: {\n","        'objective': 'multi:softprob',\n","        'eval_metric': ['mlogloss', 'merror'],\n","        'eta': 0.1,\n","        'max_depth': 6,\n","        'min_child_weight': 1,\n","        'subsample': 0.8,\n","        'colsample_bytree': 0.8,\n","        'tree_method': 'gpu_hist'  # GPU 활용\n","    })\n","\n","    # 스태킹 설정\n","    stacking_folds: int = 5\n","    use_probabilities: bool = True  # 확률값 사용 여부\n","\n","class TextBertTrack(nn.Module):\n","    \"\"\"텍스트 데이터를 처리하는 BERT 트랙\"\"\"\n","\n","    def __init__(self, config: ModelConfig, num_classes: int):\n","        super().__init__()\n","        self.config = config\n","        self.num_classes = num_classes\n","\n","        # 토크나이저 초기화\n","        self.tokenizer = AutoTokenizer.from_pretrained(\n","            config.tokenizer_name,\n","            max_length=config.max_length\n","        )\n","\n","        # BERT 모델 초기화\n","        self.bert = AutoModel.from_pretrained(config.bert_model_name)\n","\n","        # 드롭아웃 적용\n","        self.dropout = nn.Dropout(0.1)\n","\n","        # 계층적 분류기 구조\n","        self.classifier = nn.Sequential(\n","            nn.Linear(768, 512),\n","            nn.LayerNorm(512),  # 정규화 추가\n","            nn.ReLU(),\n","            nn.Dropout(0.1),\n","            nn.Linear(512, 256),\n","            nn.LayerNorm(256),\n","            nn.ReLU(),\n","            nn.Dropout(0.1),\n","            nn.Linear(256, num_classes)\n","        )\n","\n","    def forward(self, input_ids, attention_mask, return_features=False):\n","        # BERT 출력 획득\n","        outputs = self.bert(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            output_hidden_states=True  # 모든 레이어의 hidden states 획득\n","        )\n","\n","        # 마지막 4개 레이어의 [CLS] 토큰 표현을 결합\n","        last_four_layers = outputs.hidden_states[-4:]\n","        cls_embeddings = torch.stack([layer[:, 0] for layer in last_four_layers], dim=1)\n","        pooled_output = torch.mean(cls_embeddings, dim=1)\n","\n","        # 드롭아웃 적용\n","        pooled_output = self.dropout(pooled_output)\n","\n","        # 분류\n","        logits = self.classifier(pooled_output)\n","\n","        if return_features:\n","            return logits, pooled_output\n","        return logits\n","\n","class TabularXGBoostTrack:\n","    \"\"\"테이블 데이터를 처리하는 XGBoost 트랙\"\"\"\n","\n","    def __init__(self, config: ModelConfig):\n","        self.config = config\n","        self.model = None\n","        self.feature_importance = None\n","\n","    def train(self, X, y, eval_set=None):\n","        dtrain = xgb.DMatrix(X, label=y)\n","\n","        # 검증 세트가 있는 경우\n","        evals = [(dtrain, 'train')]\n","        if eval_set:\n","            deval = xgb.DMatrix(eval_set[0], label=eval_set[1])\n","            evals.append((deval, 'eval'))\n","\n","        # 학습 진행\n","        self.model = xgb.train(\n","            self.config.xgb_params,\n","            dtrain,\n","            num_boost_round=1000,\n","            evals=evals,\n","            early_stopping_rounds=50,\n","            verbose_eval=100\n","        )\n","\n","        # 특성 중요도 저장\n","        self.feature_importance = self.model.get_score(importance_type='gain')\n","\n","    def predict(self, X, return_probabilities=True):\n","        dtest = xgb.DMatrix(X)\n","        predictions = self.model.predict(dtest)\n","\n","        if not return_probabilities:\n","            predictions = predictions.argmax(axis=1)\n","        return predictions\n","\n","class StackEnsemble:\n","    \"\"\"스태킹 앙상블 모델\"\"\"\n","\n","    def __init__(self, config: ModelConfig, num_classes: int):\n","        self.config = config\n","        self.num_classes = num_classes\n","        self.bert_track = TextBertTrack(config, num_classes).to(config.device)\n","        self.xgb_track = TabularXGBoostTrack(config)\n","        self.meta_model = None\n","\n","        # 성능 측정을 위한 메트릭 초기화\n","        self.best_score = 0\n","        self.best_epoch = 0\n","\n","    def train(self, text_data, tabular_data, labels, eval_data=None):\n","        \"\"\"전체 학습 프로세스\"\"\"\n","        # BERT 트랙 학습 및 특성 추출\n","        bert_features = self._train_bert_track(text_data, labels, eval_data)\n","\n","        # XGBoost 트랙 학습 및 특성 추출\n","        xgb_features = self._train_xgb_track(tabular_data, labels, eval_data)\n","\n","        # 메타 특성 생성\n","        meta_features = self._create_meta_features(bert_features, xgb_features)\n","\n","        # 최종 메타 모델 학습\n","        self._train_meta_model(meta_features, labels)\n","\n","        # WandB에 학습 결과 기록\n","        self._log_training_results()\n","\n","    def _create_meta_features(self, bert_features, xgb_features):\n","        \"\"\"메타 특성 생성\"\"\"\n","        if self.config.use_probabilities:\n","            return np.concatenate([bert_features, xgb_features], axis=1)\n","        return np.concatenate([\n","            bert_features.argmax(axis=1).reshape(-1, 1),\n","            xgb_features.argmax(axis=1).reshape(-1, 1)\n","        ], axis=1)\n","\n","    def predict(self, text_data, tabular_data):\n","        \"\"\"예측 수행\"\"\"\n","        # 각 트랙의 예측 획득\n","        bert_preds = self._predict_bert_track(text_data)\n","        xgb_preds = self.xgb_track.predict(tabular_data)\n","\n","        # 메타 특성 생성\n","        meta_features = self._create_meta_features(bert_preds, xgb_preds)\n","\n","        # 최종 예측\n","        final_predictions = self.meta_model.predict(xgb.DMatrix(meta_features))\n","        return final_predictions\n","\n","    def _log_training_results(self):\n","        \"\"\"학습 결과를 WandB에 기록\"\"\"\n","        wandb.log({\n","            \"best_score\": self.best_score,\n","            \"best_epoch\": self.best_epoch,\n","            \"bert_feature_dim\": self.bert_track.bert.config.hidden_size,\n","            \"xgb_feature_importance\": self.xgb_track.feature_importance\n","        })"],"metadata":{"id":"Ftbe2pgIOdIH","executionInfo":{"status":"ok","timestamp":1733146280744,"user_tz":-540,"elapsed":328,"user":{"displayName":"함양훈","userId":"11408684467918551547"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import get_linear_schedule_with_warmup\n","import numpy as np\n","import pandas as pd\n","from typing import Dict, Optional, Tuple\n","from pathlib import Path\n","import logging\n","from tqdm.auto import tqdm\n","import wandb\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","import gc\n","\n","class MedicalDataset(Dataset):\n","    \"\"\"의료 데이터를 효율적으로 처리하는 데이터셋 클래스\"\"\"\n","\n","    def __init__(self,\n","                 data_path: Path,\n","                 tokenizer,\n","                 config: ModelConfig,\n","                 chunk_size: int = 1000):\n","        \"\"\"\n","        Parameters:\n","            data_path: 데이터 파일 경로\n","            tokenizer: BERT 토크나이저\n","            config: 모델 설정\n","            chunk_size: 한 번에 처리할 데이터 크기\n","        \"\"\"\n","        self.data_path = data_path\n","        self.tokenizer = tokenizer\n","        self.config = config\n","        self.chunk_size = chunk_size\n","\n","        # 데이터 인덱스 초기화\n","        self._initialize_data_index()\n","\n","    def _initialize_data_index(self):\n","        \"\"\"데이터 인덱스 구성 및 기본 검증\"\"\"\n","        try:\n","            # 데이터 기본 정보 읽기\n","            self.total_rows = sum(1 for _ in open(self.data_path)) - 1\n","\n","            # 청크 단위로 데이터 읽어오기 위한 인덱스 생성\n","            self.chunk_starts = list(range(0, self.total_rows, self.chunk_size))\n","\n","            logging.info(f\"Total rows: {self.total_rows}\")\n","            logging.info(f\"Number of chunks: {len(self.chunk_starts)}\")\n","\n","        except Exception as e:\n","            logging.error(f\"Data initialization failed: {str(e)}\")\n","            raise\n","\n","    def _load_chunk(self, chunk_idx: int) -> pd.DataFrame:\n","        \"\"\"청크 단위로 데이터 로드\"\"\"\n","        start_idx = self.chunk_starts[chunk_idx]\n","\n","        try:\n","            chunk = pd.read_csv(\n","                self.data_path,\n","                skiprows=range(1, start_idx + 1),\n","                nrows=self.chunk_size\n","            )\n","            return self._preprocess_chunk(chunk)\n","        except Exception as e:\n","            logging.error(f\"Chunk loading failed at index {chunk_idx}: {str(e)}\")\n","            raise\n","\n","    def _preprocess_chunk(self, chunk: pd.DataFrame) -> pd.DataFrame:\n","        \"\"\"데이터 청크 전처리\"\"\"\n","        # 누락된 값 처리\n","        chunk['증상'] = chunk['증상'].fillna('')\n","\n","        # 텍스트 데이터 정제\n","        chunk['증상'] = chunk['증상'].apply(self._clean_text)\n","\n","        # 수치형 특성 정규화\n","        numeric_columns = chunk.select_dtypes(include=[np.number]).columns\n","        chunk[numeric_columns] = chunk[numeric_columns].fillna(0)\n","\n","        return chunk\n","\n","    def _clean_text(self, text: str) -> str:\n","        \"\"\"텍스트 데이터 정제\"\"\"\n","        text = str(text).lower().strip()\n","        # 추가적인 텍스트 정제 규칙 적용\n","        return text\n","\n","    def __len__(self):\n","        return self.total_rows\n","\n","    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n","        \"\"\"인덱스에 해당하는 데이터 반환\"\"\"\n","        # 해당 인덱스가 속한 청크 찾기\n","        chunk_idx = idx // self.chunk_size\n","        local_idx = idx % self.chunk_size\n","\n","        try:\n","            chunk = self._load_chunk(chunk_idx)\n","            row = chunk.iloc[local_idx]\n","\n","            # 텍스트 토크나이징\n","            encoding = self._tokenize_text(row['증상'])\n","\n","            # 특성 변환\n","            features = self._prepare_features(row)\n","\n","            return {\n","                'input_ids': encoding['input_ids'],\n","                'attention_mask': encoding['attention_mask'],\n","                'features': features,\n","                'label': torch.tensor(row['주상병코드'])\n","            }\n","\n","        except Exception as e:\n","            logging.error(f\"Error processing index {idx}: {str(e)}\")\n","            raise\n","\n","    def _tokenize_text(self, text: str) -> Dict[str, torch.Tensor]:\n","        \"\"\"텍스트 토크나이징\"\"\"\n","        try:\n","            encoding = self.tokenizer(\n","                text,\n","                padding='max_length',\n","                truncation=True,\n","                max_length=self.config.max_length,\n","                return_tensors='pt'\n","            )\n","\n","            return {\n","                'input_ids': encoding['input_ids'].squeeze(0),\n","                'attention_mask': encoding['attention_mask'].squeeze(0)\n","            }\n","\n","        except Exception as e:\n","            logging.error(f\"Tokenization failed: {str(e)}\")\n","            raise\n","\n","class ModelTrainer:\n","    \"\"\"모델 학습 및 평가를 관리하는 클래스\"\"\"\n","\n","    def __init__(self,\n","                 config: ModelConfig,\n","                 model: StackEnsemble,\n","                 experiment_name: str):\n","        self.config = config\n","        self.model = model\n","        self.experiment_name = experiment_name\n","\n","        # 로깅 설정\n","        self._setup_logging()\n","\n","        # WandB 초기화\n","        self._initialize_wandb()\n","\n","    def _setup_logging(self):\n","        \"\"\"로깅 설정\"\"\"\n","        logging.basicConfig(\n","            level=logging.INFO,\n","            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n","            handlers=[\n","                logging.FileHandler(f'{self.experiment_name}.log'),\n","                logging.StreamHandler()\n","            ]\n","        )\n","\n","    def _initialize_wandb(self):\n","        \"\"\"WandB 설정\"\"\"\n","        wandb.init(\n","            project=\"medical-classification\",\n","            name=self.experiment_name,\n","            config=vars(self.config),\n","            resume=True\n","        )\n","\n","    def train(self,\n","              train_loader: DataLoader,\n","              val_loader: Optional[DataLoader] = None) -> None:\n","        \"\"\"모델 학습 수행\"\"\"\n","        try:\n","            # 옵티마이저 설정\n","            optimizer = self._setup_optimizer()\n","            scheduler = self._setup_scheduler(optimizer, len(train_loader))\n","\n","            best_val_score = float('-inf')\n","            early_stopping_counter = 0\n","\n","            for epoch in range(self.config.epochs):\n","                # 학습\n","                train_metrics = self._train_epoch(train_loader, optimizer, scheduler)\n","\n","                # 검증\n","                if val_loader is not None:\n","                    val_metrics = self._validate(val_loader)\n","                    current_val_score = val_metrics['f1_score']\n","\n","                    # 모델 저장 및 조기 종료 확인\n","                    if current_val_score > best_val_score:\n","                        best_val_score = current_val_score\n","                        self._save_model(epoch, val_metrics)\n","                        early_stopping_counter = 0\n","                    else:\n","                        early_stopping_counter += 1\n","\n","                    if early_stopping_counter >= self.config.patience:\n","                        logging.info(\"Early stopping triggered\")\n","                        break\n","\n","                # 메모리 정리\n","                gc.collect()\n","                torch.cuda.empty_cache()\n","\n","        except Exception as e:\n","            logging.error(f\"Training failed: {str(e)}\")\n","            raise\n","\n","    def _train_epoch(self,\n","                    train_loader: DataLoader,\n","                    optimizer: torch.optim.Optimizer,\n","                    scheduler) -> Dict[str, float]:\n","        \"\"\"한 에포크 학습\"\"\"\n","        self.model.train()\n","        total_loss = 0\n","        predictions = []\n","        labels = []\n","\n","        progress_bar = tqdm(train_loader, desc='Training')\n","\n","        for batch in progress_bar:\n","            try:\n","                loss, batch_preds = self._process_batch(batch, optimizer)\n","                total_loss += loss\n","\n","                predictions.extend(batch_preds.cpu().numpy())\n","                labels.extend(batch['label'].cpu().numpy())\n","\n","                scheduler.step()\n","\n","                # 진행상황 업데이트\n","                progress_bar.set_postfix({'loss': loss})\n","\n","            except Exception as e:\n","                logging.error(f\"Batch processing failed: {str(e)}\")\n","                continue\n","\n","        # 메트릭 계산\n","        metrics = self._calculate_metrics(predictions, labels)\n","        metrics['loss'] = total_loss / len(train_loader)\n","\n","        # WandB 로깅\n","        wandb.log({f'train_{k}': v for k, v in metrics.items()})\n","\n","        return metrics\n","\n","    def _validate(self, val_loader: DataLoader) -> Dict[str, float]:\n","        \"\"\"검증 수행\"\"\"\n","        self.model.eval()\n","        total_loss = 0\n","        predictions = []\n","        labels = []\n","\n","        with torch.no_grad():\n","            for batch in tqdm(val_loader, desc='Validation'):\n","                try:\n","                    loss, batch_preds = self._process_batch(batch)\n","                    total_loss += loss\n","\n","                    predictions.extend(batch_preds.cpu().numpy())\n","                    labels.extend(batch['label'].cpu().numpy())\n","\n","                except Exception as e:\n","                    logging.error(f\"Validation batch processing failed: {str(e)}\")\n","                    continue\n","\n","        # 메트릭 계산\n","        metrics = self._calculate_metrics(predictions, labels)\n","        metrics['loss'] = total_loss / len(val_loader)\n","\n","        # WandB 로깅\n","        wandb.log({f'val_{k}': v for k, v in metrics.items()})\n","\n","        return metrics\n","\n","    def _calculate_metrics(self,\n","                         predictions: np.ndarray,\n","                         labels: np.ndarray) -> Dict[str, float]:\n","        \"\"\"성능 메트릭 계산\"\"\"\n","        return {\n","            'accuracy': accuracy_score(labels, predictions),\n","            'f1_score': f1_score(labels, predictions, average='weighted'),\n","            'precision': precision_score(labels, predictions, average='weighted'),\n","            'recall': recall_score(labels, predictions, average='weighted')\n","        }\n","\n","    def _save_model(self,\n","                   epoch: int,\n","                   metrics: Dict[str, float]) -> None:\n","        \"\"\"모델 저장\"\"\"\n","        save_path = Path(f'models/{self.experiment_name}')\n","        save_path.mkdir(parents=True, exist_ok=True)\n","\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': self.model.state_dict(),\n","            'metrics': metrics\n","        }, save_path / f'model_epoch_{epoch}.pt')"],"metadata":{"id":"V2NSlTBWdHtf","executionInfo":{"status":"ok","timestamp":1733146281605,"user_tz":-540,"elapsed":436,"user":{"displayName":"함양훈","userId":"11408684467918551547"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import argparse\n","import yaml\n","from pathlib import Path\n","import logging\n","import torch\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import StratifiedKFold\n","import wandb\n","from datetime import datetime\n","from transformers import BertModel, AutoTokenizer\n","\n","class ExperimentRunner:\n","    \"\"\"전체 실험을 관리하는 클래스\"\"\"\n","\n","    def __init__(self, config_path: str):\n","        \"\"\"초기화\"\"\"\n","        self.config = self._load_config(config_path)\n","        self.experiment_name = f\"medical_classification_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}\"\n","        self._setup_logging()\n","        self._setup_paths()\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        logging.info(f\"Using device: {self.device}\")\n","\n","    def _load_config(self, config_path: str) -> dict:\n","        \"\"\"YAML 설정 파일 로드\"\"\"\n","        try:\n","            with open(config_path, 'r', encoding='utf-8') as f:\n","                return yaml.safe_load(f)\n","        except Exception as e:\n","            raise RuntimeError(f\"Failed to load config: {str(e)}\")\n","\n","    def _setup_logging(self):\n","        \"\"\"로깅 설정\"\"\"\n","        logging.basicConfig(\n","            level=logging.INFO,\n","            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n","            handlers=[\n","                logging.FileHandler(f'logs/{self.experiment_name}.log'),\n","                logging.StreamHandler()\n","            ]\n","        )\n","\n","    def _setup_paths(self):\n","        \"\"\"필요한 디렉토리 생성\"\"\"\n","        paths = ['logs', 'models', 'results', 'submissions']\n","        for path in paths:\n","            Path(path).mkdir(parents=True, exist_ok=True)\n","\n","    def _initialize_model(self):\n","        \"\"\"모델 초기화\"\"\"\n","        logging.info(\"Initializing model...\")\n","        model_config = self.config['model']\n","        model = BertModel.from_pretrained(model_config['bert_model_name'])\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_config['tokenizer_name'])\n","        return model\n","\n","    def prepare_data(self, data_path: str):\n","        \"\"\"데이터 준비 및 전처리\"\"\"\n","        try:\n","            df = pd.read_csv(data_path)\n","            self._validate_data(df)\n","\n","            # 'texts' 컬럼 생성\n","            if '증상' in df.columns:\n","                df['texts'] = df['증상']  # 'texts' 컬럼에 '증상' 값을 복사\n","            else:\n","                raise KeyError(\"'증상' 컬럼이 데이터에 없습니다.\")\n","\n","            df['dept_encoded'] = self._encode_labels(df['진료과목코드'])\n","            df['disease_encoded'] = self._encode_labels(df['주상병코드'])\n","\n","            processed_path = Path('data') / 'processed' / f'{Path(data_path).stem}_processed.csv'\n","            df.to_csv(processed_path, index=False)\n","\n","            logging.info(f\"Data preparation completed. Shape: {df.shape}\")\n","            return df\n","        except Exception as e:\n","            logging.error(f\"Data preparation failed: {str(e)}\")\n","            raise\n","\n","\n","    def _validate_data(self, df: pd.DataFrame):\n","        \"\"\"데이터 검증\"\"\"\n","        required_columns = ['진료과목코드', '주상병코드']\n","        missing_columns = [col for col in required_columns if col not in df.columns]\n","        if missing_columns:\n","            raise ValueError(f\"Missing required columns: {missing_columns}\")\n","\n","    def _encode_labels(self, column: pd.Series) -> pd.Series:\n","        \"\"\"레이블 인코딩\"\"\"\n","        from sklearn.preprocessing import LabelEncoder\n","        return LabelEncoder().fit_transform(column)\n","\n","    def _create_data_loader(self, data: dict, batch_size: int, tokenizer=None, is_training: bool = True):\n","        \"\"\"데이터 로더 생성\"\"\"\n","        tokenizer = tokenizer or self.tokenizer\n","        dataset = MedicalDataset(\n","            texts=data['texts'],\n","            labels=data['labels'],\n","            tokenizer=tokenizer,\n","            config=self.config['model']\n","        )\n","        return DataLoader(\n","            dataset,\n","            batch_size=batch_size,\n","            shuffle=is_training,\n","            num_workers=self.config['training']['num_workers']\n","        )\n","\n","    def run_experiment_with_data(self, train_df: pd.DataFrame, test_df: pd.DataFrame):\n","        \"\"\"메모리 내 데이터를 활용한 실험 실행\"\"\"\n","        try:\n","            # 'texts' 컬럼 확인\n","            if 'texts' not in train_df.columns or 'texts' not in test_df.columns:\n","                raise KeyError(\"'texts' 컬럼이 데이터프레임에 없습니다. 데이터 준비 단계를 확인하세요.\")\n","\n","            wandb.init(\n","                project=\"medical-classification\",\n","                name=self.experiment_name,\n","                config=self.config\n","            )\n","            model = self._initialize_model()\n","            model.to(self.device)\n","\n","            train_loader = self._create_data_loader(\n","                data={'texts': train_df['texts'], 'labels': train_df['dept_encoded']},\n","                batch_size=self.config['training']['batch_size'],\n","                is_training=True\n","            )\n","            test_loader = self._create_data_loader(\n","                data={'texts': test_df['texts'], 'labels': test_df['dept_encoded']},\n","                batch_size=self.config['training']['batch_size'],\n","                is_training=False\n","            )\n","\n","            loss_fn = nn.CrossEntropyLoss()\n","            optimizer = AdamW(model.parameters(), lr=self.config['model']['bert_learning_rate'])\n","\n","            logging.info(\"Starting training...\")\n","            for epoch in range(self.config['model']['epochs']):\n","                model.train()\n","                for batch in train_loader:\n","                    optimizer.zero_grad()\n","                    inputs = {key: val.to(self.device) for key, val in batch.items() if key != \"labels\"}\n","                    labels = batch[\"labels\"].to(self.device)\n","                    outputs = model(**inputs)\n","                    logits = outputs.last_hidden_state.mean(dim=1)\n","                    loss = loss_fn(logits, labels)\n","                    loss.backward()\n","                    optimizer.step()\n","                logging.info(f\"Epoch {epoch + 1}/{self.config['model']['epochs']} completed.\")\n","            logging.info(\"Training completed successfully!\")\n","\n","        except KeyError as e:\n","            logging.error(f\"KeyError during experiment: {str(e)}\")\n","            raise\n","        except Exception as e:\n","            logging.error(f\"Experiment failed: {str(e)}\")\n","            raise\n","        finally:\n","            wandb.finish()"],"metadata":{"id":"WDmFBUJ5dpak","executionInfo":{"status":"ok","timestamp":1733151453045,"user_tz":-540,"elapsed":293,"user":{"displayName":"함양훈","userId":"11408684467918551547"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["def main(train_df, test_df):\n","    \"\"\"메인 실행 함수\"\"\"\n","    config_path = \"/content/drive/MyDrive/University/4-2/정보기술학회/data/experiment.yaml\"\n","    runner = ExperimentRunner(config_path)\n","\n","    # 새로운 메서드 호출\n","    runner.run_experiment_with_data(train_df, test_df)\n","\n","    # 데이터 로딩 및 변환\n","    train_data = {\n","        \"texts\": train_df['증상'].tolist(),\n","        \"labels\": train_df['dept_encoded'].tolist()\n","    }\n","    test_data = {\n","        \"texts\": test_df['증상'].tolist(),\n","        \"labels\": test_df['dept_encoded'].tolist()\n","    }\n","\n","# 메인 함수 실행\n","if __name__ == '__main__':\n","    # 이미 전처리된 train_df와 test_df를 인자로 전달\n","    main(train_df, test_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":349},"id":"4Vz9jMiqebgo","executionInfo":{"status":"error","timestamp":1733151454132,"user_tz":-540,"elapsed":8,"user":{"displayName":"함양훈","userId":"11408684467918551547"}},"outputId":"4f2d3bc9-e7da-4c87-bb2e-10c066f86bac"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stderr","text":["ERROR:root:KeyError during experiment: \"'texts' 컬럼이 데이터프레임에 없습니다. 데이터 준비 단계를 확인하세요.\"\n"]},{"output_type":"error","ename":"KeyError","evalue":"\"'texts' 컬럼이 데이터프레임에 없습니다. 데이터 준비 단계를 확인하세요.\"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-44-47ec2904eb76>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# 이미 전처리된 train_df와 test_df를 인자로 전달\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-44-47ec2904eb76>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(train_df, test_df)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# 새로운 메서드 호출\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_experiment_with_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# 데이터 로딩 및 변환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-43-fccc8a3a6ef3>\u001b[0m in \u001b[0;36mrun_experiment_with_data\u001b[0;34m(self, train_df, test_df)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;31m# 'texts' 컬럼 확인\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'texts'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'texts'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'texts' 컬럼이 데이터프레임에 없습니다. 데이터 준비 단계를 확인하세요.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             wandb.init(\n","\u001b[0;31mKeyError\u001b[0m: \"'texts' 컬럼이 데이터프레임에 없습니다. 데이터 준비 단계를 확인하세요.\""]}]},{"cell_type":"code","source":[],"metadata":{"id":"dEhaFz-tf4nC"},"execution_count":null,"outputs":[]}]}